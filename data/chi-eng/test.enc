你好。我很高兴见到你。
深度学习是一个非常有趣的领域。
所以我通过实现一个模型来学习。
论文的标题是“Attention Is All You Need”。
作者没有使用 RNN 或 CNN，它们之前在 Seq2Seq 中使用过。
相反，它由前馈和注意力组成。
并使用正弦将句子的位置放入输入值中。
关键的注意力组织如下。
可以通过将一组查询和键值对映射到输出来描述注意力函数。
其中查询、键、值和输出都是向量。
输出计算为分配给每个值的值的加权总和，由具有该键的查询兼容性函数计算。
如果在这里增加磁头数量，速度和性能都会提高。
multi-head attention 允许模型共同关注来自不同位置的不同演示子阵列的信息。
并行能力也大大减少了训练时间。
谷歌研究博客显示了“Coreference Resolution”的出色表现。
我希望这些代码对你有帮助。
记得给仓库打星。
如果您有任何模型问题或需要改进，请随时向我发送推送请求。
这些小数据集已用韩文翻译成英文，直到这里。
感谢您阅读此数据集。
祝你一天愉快。